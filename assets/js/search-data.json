{
  
    
        "post0": {
            "title": "Fatal Police shootings in the us explanatory Analysis",
            "content": "exploration-part2 slides . Fatal Police Shootings in the US&#182; . by Fares Lassoued&#182; . Preliminary Wrangling&#182; . This dataset is from kaggle and it states as description: The 2014 killing of Michael Brown in Ferguson, Missouri, began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide. Since Jan. 1, 2015, The Washington Post has been compiling a database of every fatal shooting in the US by a police officer in the line of duty. It&#39;s difficult to find reliable data from before this period, as police killings haven&#39;t been comprehensively documented, and the statistics on police brutality are much less available. As a result, a vast number of cases go unreported. The Washington Post is tracking more than a dozen details about each killing - including the race, age and gender of the deceased, whether the person was armed, and whether the victim was experiencing a mental-health crisis. They have gathered this information from law enforcement websites, local new reports, social media, and by monitoring independent databases such as &quot;Killed by police&quot; and &quot;Fatal Encounters&quot;. The Post has also conducted additional reporting in many cases. There are four additional datasets. These are US census data on poverty rate, high school graduation rate, median household income, and racial demographics. . The dataset comes into 5 different files : PoliceKillings.csv and we will use other csv files to depict relationships and answer specific questions. . we will start by looking at the age of victims. . In&nbsp;[9]: plt.figure(figsize=[14.70, 8.27]) plt.hist(police_killings.age) plt.xlabel(&#39;age&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;count&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.title(&#39;Age distribution of killed people&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.show() . Age distribution is right skewed with a peak between 20 and 40. . Next we will plot number of deaths per month . In&nbsp;[14]: plt.figure(figsize=[14.70, 8.27]) dates = police_killings.set_index(&#39;date&#39;).groupby(pd.Grouper(freq=&#39;M&#39;))[&#39;id&#39;].count() sb.lineplot(data=dates) plt.xlabel(&#39;date&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;# of deaths per month&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.title(&#39;Shootings per month&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.xticks(); . Plotting dates of police killings between Jan 2015 and Jan 2018 shows a number of deaths between 60 and 80 for each month until May 2017 where we notice a sudden fall in number of monthly deaths below 20 per month. . In&nbsp;[15]: #body_cam plt.figure(figsize=[14.70, 8.27]) sb.countplot(data=police_killings, x=&#39;body_camera&#39;, color=uni_color) plt.title(&#39;Usage of body camera during shootings&#39;,fontsize = 14, weight = &quot;bold&quot;); plt.xlabel(&#39;body_camera&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;count&#39;,fontsize = 10, weight = &quot;bold&quot;); . In most killings there was no body camera. . In&nbsp;[17]: plt.figure(figsize=[14.70, 8.27]) sb.countplot(data=police_killings, x=&#39;race&#39;, color=uni_color, order=police_killings.race.value_counts().index) plt.title(&#39;Number of shootings per race&#39;,fontsize = 14, weight = &quot;bold&quot;); plt.xlabel(&#39;race&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;count&#39;,fontsize = 10, weight = &quot;bold&quot;); . ordered from highest to lowest counts, people with white race are the most killed in this data, followed by black as the 2nd most killed, then comes hispanic race in third place and finally Asian, Native american and other with small counts. However, we should remake this plot with each race&#39;s proportion of the whole data to be fair. . In&nbsp;[20]: plt.figure(figsize=[14.70, 8.27]) clrs = [&#39;blue&#39; if (x &lt; max(killings_per_race_count.values)) else &#39;red&#39; for x in killings_per_race_count.values ] sb.barplot(x=killings_per_race_count.index, y=killings_per_race_count.values, palette=clrs) plt.title(&#39;percentage of killings per race&#39;,fontsize = 14, weight = &quot;bold&quot;); plt.xlabel(&#39;race&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;percentage&#39;,fontsize = 10, weight = &quot;bold&quot;); . The modification of the previous plot made by dividing the number of kills per race proportion shows that black people are the most killed during this period, next comes hispanic while white is the 2nd least in the list as opposed to what the previous plot has conveyed. . In&nbsp;[22]: plt.figure(figsize=[14.70, 8.27]) sorted_counts = police_killings[&#39;signs_of_mental_illness&#39;].value_counts() plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90, counterclock = False); plt.axis(&#39;square&#39;) plt.title(&#39;Signs of mental illness&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.show() . Only around 25% of killings have shown signs of mental illness, let&#39;s check more variables like if the suspect were a fleeing or not and the threat level . In&nbsp;[23]: plt.figure(figsize=[14.70, 8.27]) most_used = police_killings.armed.value_counts()&gt;50 sb.countplot(data=police_killings.loc[police_killings.armed.isin(most_used[most_used].index.tolist())], y=&#39;armed&#39;, color=uni_color) plt.title(&#39;Most used weapons&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.xlabel(&#39;count&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;armed&#39;,fontsize = 10, weight = &quot;bold&quot;); . gun has the highest count as of &#39;armed&#39; variable, however we can see that there were around 200 unarmed and toy weapon which raises the question of how where those &#39;armed&#39; and led them to get killed? that&#39;s why we&#39;ll check another important variable: flee . In&nbsp;[25]: plt.figure(figsize=[14.70, 8.27]) plt.subplot(121) clrs_top = [&#39;blue&#39; if (x &lt; max(police_killings.state.value_counts().values)) else &#39;red&#39; for x in police_killings.state.value_counts().values ] sb.countplot(data=police_killings, y=&#39;state&#39;, palette=clrs_top[:10], order=police_killings.state.value_counts().index[:10]) plt.title(&#39;Top 10 number of shootings count per state&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.xlabel(&#39;count&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;state&#39;,fontsize = 10, weight = &quot;bold&quot;); plt.subplot(122) clrs_bottom = [&#39;blue&#39; if (x &gt; min(police_killings.state.value_counts().values)) else &#39;red&#39; for x in police_killings.state.value_counts().values ] sb.countplot(data=police_killings, y=&#39;state&#39;, palette=clrs_bottom[-10:], order=police_killings.state.value_counts().index[-10:]) plt.title(&#39;Bottom 10 number of shootings count per state&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.xlabel(&#39;count&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;&#39;,fontsize = 10, weight = &quot;bold&quot;); . California is the state where most killings took place while Rhode Island is the one with least killings. . We will compare these two states&#39; share by race, poverty level and high school grad level in the coming sections. . N.B: We will compare the mentioned states given that they have a huge difference in population (CA:39 million vs. RI: 1.1 million) . In&nbsp;[27]: #age vs. race plt.figure(figsize=[14.70, 8.27]) sb.pointplot(data = police_killings, x = &#39;race&#39;, y = &#39;age&#39;, color = uni_color, linestyles=&#39;&#39;) plt.title(&#39;Average age of killings per race&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.xlabel(&#39;race&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;avg age&#39;,fontsize = 10, weight = &quot;bold&quot;); . this plot gives a shows clearly the avg age of different races and it depicts for example that white race avg age is around 40 whilst black is around 32. . let&#39;s plot age against signs of mentall illness . In&nbsp;[28]: #age vs. signs_of_mental_illness plt.figure(figsize=[14.70, 8.27]) sb.violinplot(data = police_killings, x = &#39;signs_of_mental_illness&#39;, y = &#39;age&#39;, color = uni_color, inner=&#39;quartile&#39;) plt.title(&#39;Signs of mental illness per age&#39;,fontsize = 14, weight = &quot;bold&quot;) plt.xlabel(&#39;age&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;signs_of_mental_illness&#39;,fontsize = 10, weight = &quot;bold&quot;); . signs of mental illness appear more frequently within 30s while the distribution of ages above 50 are more larger for people showing signs of mental illness. . Now let&#39;s compare the states we mentioned earlier: Carlifornia Vs. Rhode Island . In&nbsp;[29]: #median_house_income/percentage_below_poverty_level plt.figure(figsize=[14.70, 8.27]) state_income = median_house_income.loc[median_house_income.geographic_area.isin([&#39;CA&#39;,&#39;RI&#39;])] state_poverty_lvl = percentage_below_poverty_level.loc[percentage_below_poverty_level.geographic_area.isin([&#39;CA&#39;,&#39;RI&#39;])] state_comp = percent_over25_comp_highschool.loc[percent_over25_comp_highschool.geographic_area.isin([&#39;CA&#39;,&#39;RI&#39;])] plt.subplot(131) sb.pointplot(data=state_income,x=&#39;geographic_area&#39;, y=&#39;median_income&#39;,color = uni_color, linestyles=&#39;&#39;, ci=&#39;sd&#39;) plt.xlabel(&#39;geographic_area&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;median_income&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.subplot(132) sb.pointplot(data=state_poverty_lvl,x=&#39;geographic_area&#39;, y=&#39;poverty_rate&#39;,color = uni_color, linestyles=&#39;&#39;,) plt.xlabel(&#39;geographic_area&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;poverty_rate&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.subplot(133) sb.pointplot(data=state_comp,x=&#39;geographic_area&#39;, y=&#39;percent_completed_hs&#39;,color = uni_color, linestyles=&#39;&#39;) plt.xlabel(&#39;geographic_area&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.ylabel(&#39;percent_completed_hs&#39;,fontsize = 10, weight = &quot;bold&quot;) plt.suptitle(&#39;Californa Vs Rhode Island Comparison&#39;,y = 1.04,fontsize = 14, weight = &quot;bold&quot;); . In 2015, Rhode Island cities has on average higher median_house_income and with less standard deviation than cities of California.In addition, poverty rate in RI cities is much lower that CA cities. On average, percentage of people over the age of 25 that completed high school in RI (around 88%) cities is higher than CA (around 82%) . Next we will look into weapon (among the most used weapons only) usage per state . In&nbsp;[32]: #https://stackoverflow.com/questions/45122416/one-horizontal-colorbar-for-seaborn-heatmaps-subplots-and-annot-issue-with-xtick plt.figure(figsize=[14.70, 8.27]) cat_means = police_killings.loc[police_killings.armed.isin(most_used[most_used].index.tolist())].groupby([&#39;state&#39;, &#39;armed&#39;]).count()[&#39;id&#39;] cat_means = cat_means.reset_index(name = &#39;count&#39;) cat_means = cat_means.pivot(index = &#39;armed&#39;, columns = &#39;state&#39;, values = &#39;count&#39;) sb.heatmap(cat_means,cbar_kws={&#39;orientation&#39;: &#39;vertical&#39;, &#39;label&#39; : &#39;weapons count&#39;, &quot;shrink&quot;: .30},annot=True,cmap=sb.cm.rocket_r, square=True) plt.title(&#39;Most used weapons per state&#39;,fontsize = 14, weight = &quot;bold&quot; ) plt.xlabel(&#39;state&#39;,fontsize = 10, weight = &quot;bold&quot; ) plt.ylabel(&#39;armed&#39;,fontsize = 10, weight = &quot;bold&quot; ); . We already know that gun is the most used weapon but now we can spot which states used gun the most like California (CA), Texas(TX), and states with medium gun usage like Arizona(AZ),New York(NY),et... and other states with low weapon usage like Rhode Island (RI), Vermont(VT),etc... . However, these observations are from this dataset only and we did not take each states population or number of existing guns etc... . Finally, we&#39;ll take a check the number of monthly deaths per state and race . In&nbsp;[33]: #https://stackoverflow.com/questions/25146121/extracting-just-month-and-year-separately-from-pandas-datetime-column police_killings[&#39;year-month&#39;] = police_killings.date.apply(lambda x: x.strftime(&#39;%b-%y&#39;)) #aggregate(count) by race and year-month date data = police_killings.groupby([&#39;race&#39;,&#39;year-month&#39;])[&#39;id&#39;].agg(&#39;count&#39;).reset_index().rename(columns={&#39;id&#39;:&#39;count&#39;}).sort_values(by=&#39;year-month&#39;) #custom dict to sort string dates custom_dict = {x:i for i,x in enumerate(police_killings.sort_values(by=&#39;date&#39;)[&#39;year-month&#39;].unique())} plt.figure(figsize=[14.70, 8.27]) df = data[data[&#39;race&#39;] == &#39;B&#39;] sb.lineplot(data=df.iloc[df[&#39;year-month&#39;].map(custom_dict).argsort()] , x=&#39;year-month&#39;, y=&#39;count&#39;, sort=False) df = data[data[&#39;race&#39;] == &#39;W&#39;] sb.lineplot(data=df.iloc[df[&#39;year-month&#39;].map(custom_dict).argsort()], x=&#39;year-month&#39;, y=&#39;count&#39;, sort=False) df = data[data[&#39;race&#39;] == &#39;N&#39;] sb.lineplot(data=df.iloc[df[&#39;year-month&#39;].map(custom_dict).argsort()], x=&#39;year-month&#39;, y=&#39;count&#39;, sort=False) df = data[data[&#39;race&#39;] == &#39;H&#39;] sb.lineplot(data=df.iloc[df[&#39;year-month&#39;].map(custom_dict).argsort()], x=&#39;year-month&#39;, y=&#39;count&#39;, sort=False) df = data[data[&#39;race&#39;] == &#39;A&#39;] sb.lineplot(data=df.iloc[df[&#39;year-month&#39;].map(custom_dict).argsort()], x=&#39;year-month&#39;, y=&#39;count&#39;, sort=False) df = data[data[&#39;race&#39;] == &#39;O&#39;] sb.lineplot(data=df.iloc[df[&#39;year-month&#39;].map(custom_dict).argsort()], x=&#39;year-month&#39;, y=&#39;count&#39;, sort=False) plt.xticks(rotation=25) plt.legend([&#39;B&#39;,&#39;W&#39;,&#39;N&#39;,&#39;H&#39;,&#39;A&#39;,&#39;O&#39;],loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5), title=&quot;User Type&quot;, title_fontsize = 12) plt.title(&#39;Shootings per month&#39;,fontsize = 14, weight = &quot;bold&quot; ) plt.xlabel(&#39;month&#39;,fontsize = 10, weight = &quot;bold&quot; ) plt.ylabel(&#39;kills&#39;,fontsize = 10, weight = &quot;bold&quot; ); . We can see that the time pattern of kills for white, black and hispanic is not similair but they share in common the same period of fluctuation which is 2 months. . .",
            "url": "/Blog/data%20analysis/2020/06/26/slideshow.html",
            "relUrl": "/data%20analysis/2020/06/26/slideshow.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "'WeRateDogs' twitter dataset wrangling",
            "content": "Data Wrangling: . Table of Contents . Gather | Assess | Clean | Storing Data | Predict missing dog stages | Analyze | . #collapse-hide import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from wordcloud import WordCloud, STOPWORDS import os import tweepy import json from tqdm.notebook import tqdm %matplotlib inline sns.set_style(&#39;darkgrid&#39;) pd.set_option(&#39;display.max_columns&#39;,500) . . . Gather . #make data folder if it doesn&#39;t exist path = &#39;data&#39; if not os.path.exists(path): os.makedirs(path) . Download manually the WeRateDogs twitter archive file provided by instructors twitter-archive-enhanced.csv | . twitter_archive_df = pd.read_csv(f&#39;{path}/twitter-archive-enhanced.csv&#39;) twitter_archive_df.head() . tweet_id in_reply_to_status_id in_reply_to_user_id timestamp source text retweeted_status_id retweeted_status_user_id retweeted_status_timestamp expanded_urls rating_numerator rating_denominator name doggo floofer pupper puppo . 0 892420643555336193 | NaN | NaN | 2017-08-01 16:23:56 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Phineas. He&#39;s a mystical boy. Only eve... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/892420643... | 13 | 10 | Phineas | None | None | None | None | . 1 892177421306343426 | NaN | NaN | 2017-08-01 00:17:27 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Tilly. She&#39;s just checking pup on you.... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/892177421... | 13 | 10 | Tilly | None | None | None | None | . 2 891815181378084864 | NaN | NaN | 2017-07-31 00:18:03 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Archie. He is a rare Norwegian Pouncin... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/891815181... | 12 | 10 | Archie | None | None | None | None | . 3 891689557279858688 | NaN | NaN | 2017-07-30 15:58:51 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Darla. She commenced a snooze mid meal... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/891689557... | 13 | 10 | Darla | None | None | None | None | . 4 891327558926688256 | NaN | NaN | 2017-07-29 16:00:24 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Franklin. He would like you to stop ca... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/891327558... | 12 | 10 | Franklin | None | None | None | None | . Download programmatically the tweet image predictions file image-predictions.tsv from this url: | . #collapse-hide import requests from PIL import Image from io import BytesIO def get_file(url, save=True): &quot;&quot;&quot; Downloads and save the file from the provided url and save to data/file_name, if save is False it returns the img &quot;&quot;&quot; response = requests.get(url) file_name = url.split(&#39;/&#39;)[-1] if save: try: with open(f&#39;{path}/{file_name}&#39;, mode=&#39;wb&#39;) as file: file.write(response.content) print(f&#39;{path}/{file_name} was written successfully...&#39;) except requests.exceptions.HTTPError as err: raise SystemExit(err) else: img = Image.open(BytesIO(response.content)) return img . . #get url url = &#39;https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv&#39; get_file(url) . data/image-predictions.tsv was written successfully... . #read image-predictions as pd DataFrame image_predictions_df = pd.read_csv(f&#39;{path}/image-predictions.tsv&#39;, sep=&#39; t&#39;) image_predictions_df.head() . tweet_id jpg_url img_num p1 p1_conf p1_dog p2 p2_conf p2_dog p3 p3_conf p3_dog . 0 666020888022790149 | https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg | 1 | Welsh_springer_spaniel | 0.465074 | True | collie | 0.156665 | True | Shetland_sheepdog | 0.061428 | True | . 1 666029285002620928 | https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg | 1 | redbone | 0.506826 | True | miniature_pinscher | 0.074192 | True | Rhodesian_ridgeback | 0.072010 | True | . 2 666033412701032449 | https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg | 1 | German_shepherd | 0.596461 | True | malinois | 0.138584 | True | bloodhound | 0.116197 | True | . 3 666044226329800704 | https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg | 1 | Rhodesian_ridgeback | 0.408143 | True | redbone | 0.360687 | True | miniature_pinscher | 0.222752 | True | . 4 666049248165822465 | https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg | 1 | miniature_pinscher | 0.560311 | True | Rottweiler | 0.243682 | True | Doberman | 0.154629 | True | . Scrape likes, retweet counts and additional data for each tweet_id present in the tweet archive above using Tweepy | . #read keys from env vars consumer_key = os.getenv(&#39;CONSUMER_KEY&#39;) consumer_secret = os.getenv(&#39;CONSUMER_SECRET&#39;) access_token = os.getenv(&#39;ACCESS_TOKEN&#39;) access_secret = os.getenv(&#39;ACCESS_SECRET&#39;) auth = tweepy.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_secret) api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True) . file_name = &#39;tweet_json.txt&#39; deleted_ids = [] #write each tweet to tweet_json.txt if os.path.isfile(f&#39;{path}/{file_name}&#39;): pass else: #download with open(f&#39;{path}/{file_name}&#39;, &#39;w&#39;) as file: for tweet_id in tqdm(twitter_archive_df.tweet_id.values): try: tweet = api.get_status(tweet_id,tweet_mode=&#39;extended&#39;) except: deleted_ids.append(tweet_id) tweet_str = json.dumps(tweet._json) file.write(tweet_str+&quot; n&quot;) . print(f&#39;number of deleted tweets: {len(deleted_ids)}&#39;) . number of deleted tweets: 0 . #create dataframe from tweet_json file tweet_json_df = pd.DataFrame() with open(f&#39;{path}/tweet_json.txt&#39;) as file: for line in tqdm(file): tweet = json.loads(line) tweet_json_df = tweet_json_df.append({&#39;tweet_id&#39;: tweet[&#39;id_str&#39;], &#39;retweet_count&#39;: tweet[&#39;retweet_count&#39;], &#39;favorite_count&#39;: tweet[&quot;favorite_count&quot;] }, ignore_index=True) . . tweet_json_df.head() . favorite_count retweet_count tweet_id . 0 36218.0 | 7708.0 | 892420643555336193 | . 1 31240.0 | 5698.0 | 892177421306343426 | . 2 23515.0 | 3777.0 | 891815181378084864 | . 3 39514.0 | 7872.0 | 891689557279858688 | . 4 37731.0 | 8481.0 | 891327558926688256 | . #save to csv tweet_json_df.to_csv(f&#39;{path}/tweet_extra.csv&#39;, index=False) . . Assess . dfs to Assess: . twitter_archive_df | image_predictions_df | tweet_json_df | . . Twitter_archive_df . twitter_archive_df.sample(5) . tweet_id in_reply_to_status_id in_reply_to_user_id timestamp source text retweeted_status_id retweeted_status_user_id retweeted_status_timestamp expanded_urls rating_numerator rating_denominator name doggo floofer pupper puppo . 491 813800681631023104 | NaN | NaN | 2016-12-27 17:36:16 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Sky. She&#39;s learning how to roll her R&#39;... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/813800681... | 12 | 10 | Sky | None | None | None | None | . 277 840370681858686976 | NaN | NaN | 2017-03-11 01:15:58 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | You have been visited by the magical sugar jar... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/840370681... | 13 | 10 | None | None | None | None | None | . 1240 712085617388212225 | NaN | NaN | 2016-03-22 01:16:55 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | Say hello to Olive and Ruby. They are best bud... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/712085617... | 11 | 10 | Olive | None | None | None | None | . 1273 709207347839836162 | NaN | NaN | 2016-03-14 02:39:42 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | This is Penny. She&#39;s trying on her prom dress.... | NaN | NaN | NaN | https://twitter.com/dog_rates/status/709207347... | 11 | 10 | Penny | None | None | None | None | . 2149 669684865554620416 | 6.693544e+17 | 4.196984e+09 | 2015-11-26 01:11:28 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | After countless hours of research and hundreds... | NaN | NaN | NaN | NaN | 11 | 10 | None | None | None | None | None | . #collapse-hide twitter_archive_df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2356 entries, 0 to 2355 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 tweet_id 2356 non-null int64 1 in_reply_to_status_id 78 non-null float64 2 in_reply_to_user_id 78 non-null float64 3 timestamp 2356 non-null object 4 source 2356 non-null object 5 text 2356 non-null object 6 retweeted_status_id 181 non-null float64 7 retweeted_status_user_id 181 non-null float64 8 retweeted_status_timestamp 181 non-null object 9 expanded_urls 2297 non-null object 10 rating_numerator 2356 non-null int64 11 rating_denominator 2356 non-null int64 12 name 2356 non-null object 13 doggo 2356 non-null object 14 floofer 2356 non-null object 15 pupper 2356 non-null object 16 puppo 2356 non-null object dtypes: float64(4), int64(3), object(10) memory usage: 313.0+ KB . #collapse-hide #spot retweets twitter_archive_df[(twitter_archive_df.retweeted_status_id.notnull())|(twitter_archive_df.retweeted_status_user_id.notnull())|(twitter_archive_df.retweeted_status_timestamp.notnull())] . . tweet_id in_reply_to_status_id in_reply_to_user_id timestamp source text retweeted_status_id retweeted_status_user_id retweeted_status_timestamp expanded_urls rating_numerator rating_denominator name doggo floofer pupper puppo . 19 888202515573088257 | NaN | NaN | 2017-07-21 01:02:36 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @dog_rates: This is Canela. She attempted s... | 8.874740e+17 | 4.196984e+09 | 2017-07-19 00:47:34 +0000 | https://twitter.com/dog_rates/status/887473957... | 13 | 10 | Canela | None | None | None | None | . 32 886054160059072513 | NaN | NaN | 2017-07-15 02:45:48 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @Athletics: 12/10 #BATP https://t.co/WxwJmv... | 8.860537e+17 | 1.960740e+07 | 2017-07-15 02:44:07 +0000 | https://twitter.com/dog_rates/status/886053434... | 12 | 10 | None | None | None | None | None | . 36 885311592912609280 | NaN | NaN | 2017-07-13 01:35:06 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @dog_rates: This is Lilly. She just paralle... | 8.305833e+17 | 4.196984e+09 | 2017-02-12 01:04:29 +0000 | https://twitter.com/dog_rates/status/830583320... | 13 | 10 | Lilly | None | None | None | None | . 68 879130579576475649 | NaN | NaN | 2017-06-26 00:13:58 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @dog_rates: This is Emmy. She was adopted t... | 8.780576e+17 | 4.196984e+09 | 2017-06-23 01:10:23 +0000 | https://twitter.com/dog_rates/status/878057613... | 14 | 10 | Emmy | None | None | None | None | . 73 878404777348136964 | NaN | NaN | 2017-06-24 00:09:53 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @dog_rates: Meet Shadow. In an attempt to r... | 8.782815e+17 | 4.196984e+09 | 2017-06-23 16:00:04 +0000 | https://www.gofundme.com/3yd6y1c,https://twitt... | 13 | 10 | Shadow | None | None | None | None | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1023 746521445350707200 | NaN | NaN | 2016-06-25 01:52:36 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @dog_rates: This is Shaggy. He knows exactl... | 6.678667e+17 | 4.196984e+09 | 2015-11-21 00:46:50 +0000 | https://twitter.com/dog_rates/status/667866724... | 10 | 10 | Shaggy | None | None | None | None | . 1043 743835915802583040 | NaN | NaN | 2016-06-17 16:01:16 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @dog_rates: Extremely intelligent dog here.... | 6.671383e+17 | 4.196984e+09 | 2015-11-19 00:32:12 +0000 | https://twitter.com/dog_rates/status/667138269... | 10 | 10 | None | None | None | None | None | . 1242 711998809858043904 | NaN | NaN | 2016-03-21 19:31:59 +0000 | &lt;a href=&quot;http://twitter.com/download/iphone&quot; r... | RT @twitter: @dog_rates Awesome Tweet! 12/10. ... | 7.119983e+17 | 7.832140e+05 | 2016-03-21 19:29:52 +0000 | https://twitter.com/twitter/status/71199827977... | 12 | 10 | None | None | None | None | None | . 2259 667550904950915073 | NaN | NaN | 2015-11-20 03:51:52 +0000 | &lt;a href=&quot;http://twitter.com&quot; rel=&quot;nofollow&quot;&gt;Tw... | RT @dogratingrating: Exceptional talent. Origi... | 6.675487e+17 | 4.296832e+09 | 2015-11-20 03:43:06 +0000 | https://twitter.com/dogratingrating/status/667... | 12 | 10 | None | None | None | None | None | . 2260 667550882905632768 | NaN | NaN | 2015-11-20 03:51:47 +0000 | &lt;a href=&quot;http://twitter.com&quot; rel=&quot;nofollow&quot;&gt;Tw... | RT @dogratingrating: Unoriginal idea. Blatant ... | 6.675484e+17 | 4.296832e+09 | 2015-11-20 03:41:59 +0000 | https://twitter.com/dogratingrating/status/667... | 5 | 10 | None | None | None | None | None | . 181 rows × 17 columns . #collapse-hide twitter_archive_df.isnull().sum() . . tweet_id 0 in_reply_to_status_id 2278 in_reply_to_user_id 2278 timestamp 0 source 0 text 0 retweeted_status_id 2175 retweeted_status_user_id 2175 retweeted_status_timestamp 2175 expanded_urls 59 rating_numerator 0 rating_denominator 0 name 0 doggo 0 floofer 0 pupper 0 puppo 0 dtype: int64 . twitter_archive_df.expanded_urls.isnull().sum() . 59 . #check for different url sources twitter_archive_df.expanded_urls.apply(lambda x: str(x).split(&#39;/&#39;)[2] if len(str(x).split(&#39;/&#39;))&gt;2 else x).value_counts() . twitter.com 2149 vine.co 103 www.gofundme.com 32 us.blastingnews.com 3 www.loveyourmelon.com 2 www.petfinder.com 2 m.facebook.com 1 weratedogs.com 1 www.patreon.com 1 m.youtube.com 1 goo.gl 1 gofundme.com 1 Name: expanded_urls, dtype: int64 . #check for typos in niminator and denominator twitter_archive_df[[&#39;rating_numerator&#39;, &#39;rating_denominator&#39;]].describe() . rating_numerator rating_denominator . count 2356.000000 | 2356.000000 | . mean 13.126486 | 10.455433 | . std 45.876648 | 6.745237 | . min 0.000000 | 0.000000 | . 25% 10.000000 | 10.000000 | . 50% 11.000000 | 10.000000 | . 75% 12.000000 | 10.000000 | . max 1776.000000 | 170.000000 | . shouldn&#39;t the denominator be 10 ! and max values are flying!! . #collapse-hide twitter_archive_df.rating_denominator.value_counts() . . 10 2333 11 3 50 3 80 2 20 2 2 1 16 1 40 1 70 1 15 1 90 1 110 1 120 1 130 1 150 1 170 1 7 1 0 1 Name: rating_denominator, dtype: int64 . #collapse-hide twitter_archive_df.rating_numerator.value_counts() . . 12 558 11 464 10 461 13 351 9 158 8 102 7 55 14 54 5 37 6 32 3 19 4 17 1 9 2 9 420 2 0 2 15 2 75 2 80 1 20 1 24 1 26 1 44 1 50 1 60 1 165 1 84 1 88 1 144 1 182 1 143 1 666 1 960 1 1776 1 17 1 27 1 45 1 99 1 121 1 204 1 Name: rating_numerator, dtype: int64 . #collapse-hide twitter_archive_df.dtypes . . tweet_id int64 in_reply_to_status_id float64 in_reply_to_user_id float64 timestamp object source object text object retweeted_status_id float64 retweeted_status_user_id float64 retweeted_status_timestamp object expanded_urls object rating_numerator int64 rating_denominator int64 name object doggo object floofer object pupper object puppo object dtype: object . image_predictions_df . image_predictions_df.sample(5) . tweet_id jpg_url img_num p1 p1_conf p1_dog p2 p2_conf p2_dog p3 p3_conf p3_dog . 1542 791312159183634433 | https://pbs.twimg.com/media/CvtONV4WAAAQ3Rn.jpg | 4 | miniature_pinscher | 0.892925 | True | toy_terrier | 0.095524 | True | Doberman | 0.003544 | True | . 378 673240798075449344 | https://pbs.twimg.com/media/CVfU7KLXAAAAgIa.jpg | 1 | Airedale | 0.443004 | True | brown_bear | 0.114162 | False | Chesapeake_Bay_retriever | 0.094639 | True | . 1594 798697898615730177 | https://pbs.twimg.com/media/CeRoBaxWEAABi0X.jpg | 1 | Labrador_retriever | 0.868671 | True | carton | 0.095095 | False | pug | 0.007651 | True | . 260 670783437142401025 | https://pbs.twimg.com/media/CU8Z-OxXAAA-sd2.jpg | 1 | lacewing | 0.381955 | False | sulphur_butterfly | 0.106810 | False | leafhopper | 0.068347 | False | . 1607 800513324630806528 | https://pbs.twimg.com/media/Cxv-nkJUoAAhzMt.jpg | 1 | Pembroke | 0.828904 | True | Cardigan | 0.167373 | True | Chihuahua | 0.000766 | True | . #collapse-hide image_predictions_df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2075 entries, 0 to 2074 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 tweet_id 2075 non-null int64 1 jpg_url 2075 non-null object 2 img_num 2075 non-null int64 3 p1 2075 non-null object 4 p1_conf 2075 non-null float64 5 p1_dog 2075 non-null bool 6 p2 2075 non-null object 7 p2_conf 2075 non-null float64 8 p2_dog 2075 non-null bool 9 p3 2075 non-null object 10 p3_conf 2075 non-null float64 11 p3_dog 2075 non-null bool dtypes: bool(3), float64(3), int64(2), object(4) memory usage: 152.1+ KB . image_predictions_df.describe() . tweet_id img_num p1_conf p2_conf p3_conf . count 2.075000e+03 | 2075.000000 | 2075.000000 | 2.075000e+03 | 2.075000e+03 | . mean 7.384514e+17 | 1.203855 | 0.594548 | 1.345886e-01 | 6.032417e-02 | . std 6.785203e+16 | 0.561875 | 0.271174 | 1.006657e-01 | 5.090593e-02 | . min 6.660209e+17 | 1.000000 | 0.044333 | 1.011300e-08 | 1.740170e-10 | . 25% 6.764835e+17 | 1.000000 | 0.364412 | 5.388625e-02 | 1.622240e-02 | . 50% 7.119988e+17 | 1.000000 | 0.588230 | 1.181810e-01 | 4.944380e-02 | . 75% 7.932034e+17 | 1.000000 | 0.843855 | 1.955655e-01 | 9.180755e-02 | . max 8.924206e+17 | 4.000000 | 1.000000 | 4.880140e-01 | 2.734190e-01 | . image_predictions_df.isnull().sum() . tweet_id 0 jpg_url 0 img_num 0 p1 0 p1_conf 0 p1_dog 0 p2 0 p2_conf 0 p2_dog 0 p3 0 p3_conf 0 p3_dog 0 dtype: int64 . #check for added probabilities &gt; 1 image_predictions_df[image_predictions_df[[&#39;p1_conf&#39;, &#39;p2_conf&#39;, &#39;p3_conf&#39;]].sum(axis=1)&gt;1] . tweet_id jpg_url img_num p1 p1_conf p1_dog p2 p2_conf p2_dog p3 p3_conf p3_dog . 106 667866724293877760 | https://pbs.twimg.com/media/CUS9PlUWwAANeAD.jpg | 1 | jigsaw_puzzle | 1.0 | False | prayer_rug | 1.011300e-08 | False | doormat | 1.740170e-10 | False | . #check for img types image_predictions_df.jpg_url.apply(lambda x: x.split(&#39;.&#39;)[-1]).value_counts() . jpg 2073 png 2 Name: jpg_url, dtype: int64 . tweet_json_df . tweet_json_df.sample(5) . favorite_count retweet_count tweet_id . 1731 2350.0 | 755.0 | 679844490799091713 | . 437 20090.0 | 6393.0 | 820078625395449857 | . 813 6307.0 | 1458.0 | 771102124360998913 | . 2207 304.0 | 109.0 | 668627278264475648 | . 2121 537.0 | 210.0 | 670408998013820928 | . tweet_json_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2356 entries, 0 to 2355 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 favorite_count 2356 non-null float64 1 retweet_count 2356 non-null float64 2 tweet_id 2356 non-null object dtypes: float64(2), object(1) memory usage: 55.3+ KB . tweet_json_df.dtypes . favorite_count float64 retweet_count float64 tweet_id object dtype: object . tweet_json_df.isnull().sum() . favorite_count 0 retweet_count 0 tweet_id 0 dtype: int64 . Quality: . twitter_archive_df: . html tag &lt;a href ...&gt; in source column | columns with relatively large missing entries in_reply_to_..., retweeted_status_... | missing expanded_urls | denominator/numerator typos | time columns object dtypes | floofer column misspelled | missing dog names | There are 181 retweets | . image_predictions_df: . jpg_url column name while there are png imgages | . tweet_json_df: . favorite_count and retweet_count dtypes are float | . Tidiness: . image_predictions_df is a separate table | tweet_json_df is a separate table | dog types are in different columns | . . Clean . twitter_archive_df = pd.read_csv(f&#39;{path}/twitter-archive-enhanced.csv&#39;) image_predictions_df = pd.read_csv(f&#39;{path}/image-predictions.tsv&#39;, sep=&#39; t&#39;) tweet_json_df = pd.read_csv(f&#39;{path}/tweet_extra.csv&#39;) . twitter_arch_clean = twitter_archive_df.copy() img_pred_clean = image_predictions_df.copy() tweet_json_clean = tweet_json_df.copy() . twitter_archive_df: . Define . Drop retweets by dropping the rows where retweets are not null | . Code . #drop retweets #get indexes of rows to drop ids_to_drop = twitter_archive_df[(twitter_archive_df.retweeted_status_id.notnull())|(twitter_archive_df.retweeted_status_user_id.notnull())|(twitter_archive_df.retweeted_status_timestamp.notnull())].index #drop twitter_arch_clean = twitter_arch_clean.drop(ids_to_drop) . Test . #test for retweets assert twitter_arch_clean.retweeted_status_id.isnull().all() assert twitter_arch_clean.retweeted_status_timestamp.isnull().all() assert twitter_arch_clean.retweeted_status_user_id.isnull().all() . Define . remove html &lt;a href tag..&gt; for source column | . Code . #remove html ahref tag from source column twitter_arch_clean.source = twitter_arch_clean.source.apply(lambda x: x.split(&#39;&gt;&#39;)[1].split(&#39;&lt;&#39;)[0]) . Test . #test for source twitter_arch_clean.source.unique() . array([&#39;Twitter for iPhone&#39;, &#39;Twitter Web Client&#39;, &#39;Vine - Make a Scene&#39;, &#39;TweetDeck&#39;], dtype=object) . Define . drop columns with relatively large missing entries in_reply_to_..., retweeted_status_... | . Code . #drop columns with missing entries cols_to_drop = [&#39;in_reply_to_status_id&#39;, &#39;in_reply_to_user_id&#39;, &#39;retweeted_status_id&#39;, &#39;retweeted_status_user_id&#39;, &#39;retweeted_status_timestamp&#39;] twitter_arch_clean = twitter_arch_clean.drop(cols_to_drop, axis=1) . Test . #test for dropped cols: test if there are columns with large (1000) missing entries assert (twitter_arch_clean.isnull().sum()&gt;1000).all() == False . Define . fix numerators and denominators by extracting values with decimals from text and removing outliers | . Code . def remove_outliers(col): &quot;&quot;&quot; remove outliers by selecting values between q_low and q_high &quot;&quot;&quot; q_low = col.quantile(0.01) q_high = col.quantile(0.99) return col[(col&gt;=q_low)&amp;(col&lt;=q_high)] . #extract numerator and denominator from text column extracted_values = twitter_archive_df.text.str.extract(&#39;((?: d+ .)? d+) /(( d+ .)? d+)&#39;, expand=True) #get numerator numerator = extracted_values.loc[:,0].astype(float) #get denominator denominator = extracted_values.loc[:,1].astype(float) #set values to df twitter_arch_clean.rating_numerator = remove_outliers(numerator) twitter_arch_clean.rating_denominator = remove_outliers(denominator) . Test . #test for numerator and denominator twitter_arch_clean[[&#39;rating_numerator&#39;, &#39;rating_denominator&#39;]].describe() . rating_numerator rating_denominator . count 2133.000000 | 2153.0 | . mean 10.692583 | 10.0 | . std 2.032640 | 0.0 | . min 3.000000 | 10.0 | . 25% 10.000000 | 10.0 | . 50% 11.000000 | 10.0 | . 75% 12.000000 | 10.0 | . max 14.000000 | 10.0 | . Define . drop missing expanded_urls | . Code . #drop rows with missing urls twitter_arch_clean = twitter_arch_clean.dropna() . Test . #test with missing urls assert twitter_arch_clean.expanded_urls.isnull().sum() == 0 . Define . rename floofer column and its values to floof | . Code . #rename floofer column to floof twitter_arch_clean = twitter_arch_clean.rename({&#39;floofer&#39;:&#39;floof&#39;}, axis=1) #replace values twitter_arch_clean.floof = twitter_arch_clean.floof.str.replace(&#39;floofer&#39;,&#39;floof&#39;) . Test . #test for replaced column name floof assert &#39;floofer&#39; not in twitter_arch_clean.columns . Define . convert timestamp to date column | . Code . #convert timestamp to date column twitter_arch_clean.timestamp = pd.to_datetime(twitter_arch_clean.timestamp) . Test . #convert timestamp to date column twitter_arch_clean.timestamp.dtype . datetime64[ns, UTC] . Define . group dog stages into one column by combining different columns | . Code . #collapse-hide def merge_dogstages(df): &quot;&quot;&quot; replace every missing and None dog_stage with empty space and combine different dog stages together for each row. &quot;&quot;&quot; cols = [&#39;doggo&#39;, &#39;floof&#39;, &#39;pupper&#39;, &#39;puppo&#39;] for col in cols: df[col] = df[col].replace(&#39;None&#39;, &#39;&#39;) df[col] = df[col].replace(np.NaN, &#39;&#39;) df[&#39;stage&#39;] = df.doggo + df.floof + df.pupper + df.puppo df.loc[df.stage == &#39;doggopupper&#39;, &#39;stage&#39;] = &#39;doggo, pupper&#39; df.loc[df.stage == &#39;doggopuppo&#39;, &#39;stage&#39;] = &#39;doggo, puppo&#39; df.loc[df.stage == &#39;doggofloof&#39;, &#39;stage&#39;] = &#39;doggo, floof&#39; df[&#39;stage&#39;] = df[&#39;stage&#39;].replace(&#39;&#39;, &#39;None&#39;) return df.drop(cols, axis=1) . . twitter_arch_clean = merge_dogstages(twitter_arch_clean) . Test . #test for group dog stages into one column using melt twitter_arch_clean.stage.value_counts() . None 1744 pupper 222 doggo 72 puppo 23 doggo, pupper 10 floof 9 doggo, floof 1 doggo, puppo 1 Name: stage, dtype: int64 . . image_predictions_df: . Define . change jpg_url column name to jpg because there are .png pictures | . Code . img_pred_clean = img_pred_clean.rename({&#39;jpg_url&#39;:&#39;img_url&#39;}, axis=1) . Test . #test for url column name assert &#39;jpg_url&#39; not in img_pred_clean.columns . Define . merge ima_pred_df with twitter_arch_clean | . Code . #merge dfs twitter_arch_clean = pd.merge(twitter_arch_clean, img_pred_clean, on=&#39;tweet_id&#39;) . Test . . tweet_json_clean: . tweet_json_clean.head() . favorite_count retweet_count tweet_id . 0 36218.0 | 7708.0 | 892420643555336193 | . 1 31240.0 | 5698.0 | 892177421306343426 | . 2 23515.0 | 3777.0 | 891815181378084864 | . 3 39514.0 | 7872.0 | 891689557279858688 | . 4 37731.0 | 8481.0 | 891327558926688256 | . Define . convert favorite_count and retweet_count dtypes to integer | . Code . #reset order (optional) tweet_json_clean = tweet_json_clean[[&#39;tweet_id&#39;, &#39;favorite_count&#39;, &#39;retweet_count&#39;]] #convert dtypes tweet_json_clean.favorite_count = tweet_json_clean.favorite_count.astype(&#39;int32&#39;) tweet_json_clean.retweet_count = tweet_json_clean.retweet_count.astype(&#39;int32&#39;) . Test . #test for column dtypes tweet_json_clean[[&#39;favorite_count&#39;, &#39;retweet_count&#39;]].dtypes . favorite_count int32 retweet_count int32 dtype: object . Define . merge tweet_json_df with twitter_arch_clean dataframe | . Code . #merge dfs twitter_arch_clean = pd.merge(twitter_arch_clean, tweet_json_clean, on=&#39;tweet_id&#39;) . Test . . Storing data . #twitter_archive_master twitter_arch_clean.to_csv(&#39;data/clean/twitter_archive_master.csv&#39;, index=False) . #store to sqlite db(second option) from sqlalchemy import create_engine engine = create_engine(&#39;sqlite:///data/clean/twitter_archive.db&#39;, echo=False) twitter_arch_clean.to_sql(&#39;twitter_archive&#39;, con=engine) . . Predict missing dog_stages . We will try to use the data we have and try to predict the missing dog_stages, so let&#39;s give it a try . df = pd.read_csv(&#39;data/clean/twitter_archive_master.csv&#39;) . #save indx to use later indx = df.index . there are 1665 values to predict of four classes, so our problem is a multiclass classification problem . #let&#39;s define our train and test data train = df[df.stage != &#39;None&#39;] test = df[df.stage == &#39;None&#39;] print(f&#39;train: {train.shape}, test: {test.shape}&#39;) . train: (310, 22), test: (1665, 22) . #collapse-hide #oversampling from imblearn.over_sampling import SMOTE #drop rows with class of value count = 1 single_class = train[(train.stage==&#39;doggo, floof&#39;)|(train.stage==&#39;doggo, puppo&#39;)] to_drop = single_class.index train_le = train.drop(to_drop) stages = pd.factorize(train.stage)[1] for col in train_le.select_dtypes(include=&#39;object&#39;).columns: train_le[col] = pd.factorize(train_le[col])[0] smote = SMOTE(sampling_strategy=&#39;auto&#39;, n_jobs=-1) X_sm, y_sm = smote.fit_resample(train_le.drop(&#39;stage&#39;, axis=1), train_le[&#39;stage&#39;]) df_oversampled = pd.DataFrame(X_sm, columns=train_le.drop(&#39;stage&#39;, axis=1).columns) df_oversampled[&#39;target&#39;] = y_sm #df_oversampled[&#39;target&#39;].value_counts().plot(kind=&#39;bar&#39;, title=&#39;Count (target)&#39;); . . #collapse-hide import lightgbm as lgb from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split , StratifiedKFold features_to_drop = [&#39;target&#39;,&#39;text&#39;,&#39;source&#39;, &#39;img_url&#39;, &#39;rating_denominator&#39;, &#39;expanded_urls&#39;] features = set(df_oversampled.columns) - set(features_to_drop) X = df_oversampled[features] y = df_oversampled.target #label encode test test_le = test.copy() for col in test_le.select_dtypes(include=&#39;object&#39;).columns: test_le[col] = pd.factorize(test_le[col])[0] skf = StratifiedKFold(n_splits=5) preds = np.zeros((len(test),5), dtype=object) scores = [] seed = 42 lgb_clf = lgb.LGBMClassifier(random_state=seed) for i, (train_index, val_index) in enumerate(tqdm(skf.split(X,y))): x_train, x_val = X.loc[train_index], X.loc[val_index] y_train, y_val = y.loc[train_index], y.loc[val_index] lgb_clf.fit(x_train, y_train) pred = lgb_clf.predict(x_val) score = accuracy_score(y_val, pred) print(f&#39;fold{i+1} accuracy: {score}&#39;) scores.append(score) preds[:,i] = lgb_clf.predict(test_le[features]).squeeze() print(f&#39;Avg Accuracy: {np.mean(scores)}&#39;) . . fold1 accuracy: 0.7401960784313726 fold2 accuracy: 0.8333333333333334 fold3 accuracy: 0.9607843137254902 fold4 accuracy: 0.9705882352941176 fold5 accuracy: 0.9705882352941176 Avg Accuracy: 0.8950980392156863 . I used a 5 fold cross validation to predict the stage and saved the results to preds to use the pick the mode of each prediction, meaning for each single value we predict 5 times and get the most occured pred. . from scipy.stats import mode predictions = mode(preds, axis=1).mode . #assign to stage test.stage = predictions #restore original values def original_values(x): if x==0: x=stages[0] elif x==1: x=stages[1] elif x==2: x=stages[2] elif x==3: x=stages[3] else: x=stages[4] return x test.stage = test.stage.apply(original_values) . /home/zowlex/anaconda3/envs/ml/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . #concat results df = pd.concat([train, test]).set_index(indx) . . Analyze . df.head() . tweet_id timestamp source text expanded_urls rating_numerator rating_denominator name stage img_url img_num p1 p1_conf p1_dog p2 p2_conf p2_dog p3 p3_conf p3_dog favorite_count retweet_count . 0 890240255349198849 | 2017-07-26 15:59:51+00:00 | Twitter for iPhone | This is Cassie. She is a college pup. Studying... | https://twitter.com/dog_rates/status/890240255... | 14.0 | 10.0 | Cassie | doggo | https://pbs.twimg.com/media/DFrEyVuW0AAO3t9.jpg | 1 | Pembroke | 0.511319 | True | Cardigan | 0.451038 | True | Chihuahua | 0.029248 | True | 29900 | 6697 | . 1 889665388333682689 | 2017-07-25 01:55:32+00:00 | Twitter for iPhone | Here&#39;s a puppo that seems to be on the fence a... | https://twitter.com/dog_rates/status/889665388... | 13.0 | 10.0 | None | puppo | https://pbs.twimg.com/media/DFi579UWsAAatzw.jpg | 1 | Pembroke | 0.966327 | True | Cardigan | 0.027356 | True | basenji | 0.004633 | True | 45079 | 9126 | . 2 889531135344209921 | 2017-07-24 17:02:04+00:00 | Twitter for iPhone | This is Stuart. He&#39;s sporting his favorite fan... | https://twitter.com/dog_rates/status/889531135... | 13.0 | 10.0 | Stuart | puppo | https://pbs.twimg.com/media/DFg_2PVW0AEHN3p.jpg | 1 | golden_retriever | 0.953442 | True | Labrador_retriever | 0.013834 | True | redbone | 0.007958 | True | 14198 | 2057 | . 3 886366144734445568 | 2017-07-15 23:25:31+00:00 | Twitter for iPhone | This is Roscoe. Another pupper fallen victim t... | https://twitter.com/dog_rates/status/886366144... | 12.0 | 10.0 | Roscoe | pupper | https://pbs.twimg.com/media/DE0BTnQUwAApKEH.jpg | 1 | French_bulldog | 0.999201 | True | Chihuahua | 0.000361 | True | Boston_bull | 0.000076 | True | 19840 | 2891 | . 4 884162670584377345 | 2017-07-09 21:29:42+00:00 | Twitter for iPhone | Meet Yogi. He doesn&#39;t have any important dog m... | https://twitter.com/dog_rates/status/884162670... | 12.0 | 10.0 | Yogi | doggo | https://pbs.twimg.com/media/DEUtQbzW0AUTv_o.jpg | 1 | German_shepherd | 0.707046 | True | malinois | 0.199396 | True | Norwegian_elkhound | 0.049148 | True | 19147 | 2733 | . What is/are the factor(s) of most rating dogs? . #get the most rated dogs most_rated = df[df.rating_numerator==df.rating_numerator.max()] most_rated.shape . (36, 22) . #collapse-hide #plot dog stage of most rated dogs plt.figure(figsize=(8,6)) most_rated.groupby(&#39;stage&#39;).size().plot(kind=&#39;bar&#39;) plt.ylabel(&#39;# dog stages&#39;) plt.legend([&#39;rating=14/10&#39;]) plt.title(&#39;dog stages count of most rated dogs&#39;) plt.show() . . We can see that pupper stage is the most occuring in this dataset, let&#39;s dig deeper . #collapse-hide #plot tweet favotrite count of most rated dogs plt.figure(figsize=(8,6)) most_rated.groupby(&#39;stage&#39;)[&#39;favorite_count&#39;].sum().plot(kind=&#39;pie&#39;, autopct=&#39;%1.1f%%&#39;) plt.ylabel(&#39;&#39;) plt.title(&#39;Tweet favorite counts per dog stage &#39;); . . Pupper dog stage has the most favorite count among the most rated dogs, then come doggo and lastly puppo and floof. . #collapse-hide #visualize 4 different dog stages pupper_url = most_rated[most_rated.stage==&#39;pupper&#39;].iloc[1].img_url doggo_url = most_rated[most_rated.stage==&#39;doggo&#39;].iloc[0].img_url puppo_url = most_rated[most_rated.stage==&#39;puppo&#39;].iloc[0].img_url floof_url = most_rated[most_rated.stage==&#39;floof&#39;].iloc[0].img_url #download images pupper_img = get_file(pupper_url, save=False) doggo_img = get_file(doggo_url, save=False) puppo_img = get_file(puppo_url, save=False) floof_img = get_file(floof_url, save=False) #subplots fig, ax = plt.subplots(1,4, figsize=(20,10)) ax[0].imshow(pupper_img) ax[0].set_title(&#39;pupper&#39;) ax[0].axis(&#39;off&#39;) ax[1].imshow(doggo_img) ax[1].set_title(&#39;doggo&#39;) ax[1].axis(&#39;off&#39;) ax[2].imshow(puppo_img) ax[2].set_title(&#39;puppo&#39;) ax[2].axis(&#39;off&#39;) ax[3].imshow(floof_img) ax[3].set_title(&#39;floof&#39;) ax[3].axis(&#39;off&#39;) plt.show() . . #GEt the most predicted dog breed df = most_rated.groupby(&#39;p1&#39;)[&#39;p1_conf&#39;,&#39;p1_dog&#39;].mean().sort_values(&#39;p1_conf&#39;, ascending=False) #exclude false predictions df[df.p1_dog==1] . /home/zowlex/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead. . p1_conf p1_dog . p1 . Pomeranian 0.960199 | True | . Gordon_setter 0.940724 | True | . Chihuahua 0.876543 | True | . black-and-tan_coonhound 0.854861 | True | . Old_English_sheepdog 0.798481 | True | . bloodhound 0.777562 | True | . French_bulldog 0.774122 | True | . golden_retriever 0.761221 | True | . Rottweiler 0.681495 | True | . Pembroke 0.621747 | True | . Eskimo_dog 0.596045 | True | . Irish_setter 0.505496 | True | . Bedlington_terrier 0.392535 | True | . standard_poodle 0.351308 | True | . Samoyed 0.281463 | True | . Lakeland_terrier 0.275242 | True | . #collapse-hide #plot dog stage of most rated dogs plt.figure(figsize=(10,7)) plt.bar(x=df[df.p1_dog==1].index, height=df[df.p1_dog==1].p1_conf) plt.xticks(rotation=90) plt.ylabel(&#39;predicted proba&#39;) plt.title(&#39;Top predictions of most rated dogs&#39;); . . Top 3 predicted dog breeds are: . Pomeranian | Gordon Setter | Chihuahua | . Visualize tweets wordcloud . #collapse-hide stopwords = set(STOPWORDS) wordcloud = WordCloud( background_color=&#39;black&#39;, stopwords=stopwords, max_words=2000, max_font_size=60, random_state=42, #mask=img_mask, contour_width=3, contour_color=&#39;steelblue&#39; ).generate(str(most_rated[&#39;text&#39;])) plt.figure(dpi=900) fig = plt.figure(1) plt.imshow(wordcloud) plt.axis(&#39;off&#39;) plt.show() . . . Resources . correct way to raise try/except using requests . | setup env variables for API keys and tokens in conda/linux . | wordcloud . | .",
            "url": "/Blog/data%20analysis/machine%20learning/2020/06/15/Twitter-dataset-wrangling.html",
            "relUrl": "/data%20analysis/machine%20learning/2020/06/15/Twitter-dataset-wrangling.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "John Conways' game of life in python",
            "content": "Introduction . . This small project represents the python Implementation of John Conways’ game of life, if you don’t know what I’m talking about stick with me, I’ll explain everything from idea to implementation. . Project repo: Game Of Life . What is the game of life . According to Wikipedia the game of life is in simple terms a game that mimics the behavior and lifecycle of biological cells in nature following 4 simple rules: . Any live cell with fewer than two live neighbours dies, as if by underpopulation. | Any live cell with two or three live neighbours lives on to the next generation. | Any live cell with more than three live neighbours dies, as if by overpopulation. | Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction. | Out of these simple rules we can create starting configurations that lead to complex systems and shapes like oscillators, spaceships and even turing machines . Motivation . I was glad that I stumbled upon a great talk on youtube called The Art of Code which I highly recommend watching it. Dylan Beattie is a programmer and musician talked about different beautiful artistic moments of coding and what coders have achieved from writing mesmerizing quines to coming up with weird but funny programming languages like Rockstar programming language but what has really caught my attention is talking about the game of life which made my jaw drop and let me think very deeply about its capabilities. At this moment I knew nothing about it and suddenly had the urge to implement it in my favourite programming language (python) . Implementation . Engine . At this point I have no idea how to implement this graphically so I started first by creating a Grid class which represents our infinte 2d space. This grid is in fact a 2d array which translates to a nested list in python. our grid of length w and width h is initialized by w*h zeros when called. . . N.B: 0 means that the cell is dead and 1 is alive class Grid: . import random class Grid(list): def __init__(self, w, h): &quot;&quot;&quot; initializes a grid of zeros with length w and width h &quot;&quot;&quot; self.w = w self.h = h super() for row in range(w): # Add an empty array that will hold each cell # in this row self.append([]) for column in range(h): self[row].append(0) # Append a cell def show(self): for row in range(self.w): ch=&#39;[&#39; for col in range(self.h): ch += str(self[row][col])+&#39;,&#39; if col == self.h-1: ch=ch[:-1]+&#39;]&#39; print(ch,&#39; n&#39;) . Example: . grid = Grid(5,5) grid.show() . Output: . [0,0,0] . [0,0,0] . [0,0,0] . Now that we have defined and created our data structure the next task is to define a function to calculate the number of alive neighbors for each cell of position x,y by visiting its next eight neighbors and summing up their values. . def alive_neighbors(self,x,y): &quot;&quot;&quot; returns the number of alive neighbors (=1) for a certain x,y position &quot;&quot;&quot; res = 0 for row in range(x-1,x+2): for col in range(y-1,y+2): if (row==x) and (col==y): continue try: if self[row][col] == 1: res+=1 pass except IndexError:#If this error is raised for cells on the edges we consider the next edge cells are the neighbors, like we are wrapping a sheet of paper so the edges touch row = (x+row+self.w)%self.w col = (y+col+self.h)%self.h return res . Example: . [0,1,0] . [0,0,0] . [1,1,1] . neighbors = grid.alive_neighbors(1,1) # cell in position 1,1 print(neighbors) . Output: 4 which is the sum of neighbor alive cells . Before thinking about gui we have to try it on terminal and see if everything works fine. We start by making a grid and choose starting configuration, get each cell’s state and update it in next_grid, after looping through all cells we print the updated version (next_grid) and each loop through the grid represnts a new generation of cells. . test.py . from grid import Grid grid = Grid(3,5) grid[1][1] = 1 grid[1][2] = 1 grid[1][3] = 1 if __name__ == &#39;__main__&#39;: grid.show() next_grid = Grid(3,5) for row in range(3): for col in range(5): state = grid[row][col] #get every element&#39;s neighbors of grid neighbors = grid.alive_neighbors(row,col) if neighbors == 3 and state ==0 :# These are the first 3 rules stated above next_grid[row][col] = 1 elif (neighbors&lt;2 or neighbors&gt;3) and state ==1 : # This is the last rule of game of life next_grid[row][col] = 0 else: next_grid[row][col] = state print(&#39;=&#39;*10) next_grid.show() grid = next_grid . Output: this is the representation of an oscillator . [0,0,0,0,0] ===&gt; [0,0,1,0,0] . [0,1,1,1,0] ===&gt; [0,0,1,0,0] . [0,0,0,0,0] ===&gt; [0,0,1,0,0] . Interface . For the gui I decided to use pygame for its simplicity and powerful functionalities, we start by defining some global variables. . # Define some colors BLACK = (0, 0, 0) WHITE = (255, 255, 255) GREEN = (0, 255, 0) RED = (255, 0, 0) # This sets the WIDTH and HEIGHT of each grid location SQUARE = 20 # This sets the margin between each cell MARGIN = 2 #Screen resolution w = 700 h = 500 #calculate row_num, col_num from current resolution row_num = w//SQUARE col_num = h//SQUARE . Next define the draw method which represents our grid of zeros and ones by graphical squares of white color if dead and green color if alive: . #def initialize grid def draw_grid(row_num, col_num, g): # Draw the grid for row in range(row_num): for column in range(col_num): color = WHITE pygame.draw.rect(screen, color, [(MARGIN + SQUARE) * row + MARGIN, (MARGIN + SQUARE) * column + MARGIN, SQUARE, SQUARE]) if g[row][column] == 1: pygame.draw.rect(screen, GREEN, [(MARGIN + SQUARE) * row + MARGIN, (MARGIN + SQUARE) * column + MARGIN, SQUARE, SQUARE]) pygame.display.flip() . Then we have to create a window of size w,h and draw initial grid . def create_window(w, h): # Set the HEIGHT and WIDTH of the screen screen = pygame.display.set_mode([w,h]) # Set title of screen pygame.display.set_caption(&quot;John Conway&#39;s game of life&quot;) return screen #create window of size 700px by 500px screen = create_window(w, h) #draw initial grid draw_grid(row_num, col_num, grid) . The final step is defining the main loop to keep the game running and see some magic patterns, once we press the space key we apply the same rules on the grid like we did on the first prototype of the game and draw each new generation of cells to the screen of our window. . # -- Main Program Loop -- while not done: for event in pygame.event.get(): # User did something if event.type == pygame.QUIT: # If user clicked close done = True # Flag that we are done so we exit this loop elif event.type == pygame.KEYDOWN: # Game starts if event.key == K_SPACE: print(&#39;space key pressed&#39;) x = 0 while True: next_grid = Grid(row_num, col_num) for row in range(row_num): for col in range(col_num): state = grid[row][col]#current grid state 0 or 1 neighbors = grid.alive_neighbors(row,col)#current grid alive neighbors if state == 1 and (neighbors&lt;2 or neighbors&gt;3): next_grid[row][col] = 0 elif state == 0 and neighbors == 3: next_grid[row][col] = 1 else: next_grid[row][col] = state draw_grid(row_num, col_num, next_grid) grid = next_grid x+=1 print(&#39;generation:&#39;,x) # Limit to x frames per second clock.tick(x) # Be IDLE friendly. If you forget this line, the program will &#39;hang&#39; # on exit. pygame.quit() . Final Result of a random config: . . Conclusion . I had so fun working on this project and I hope it helps you because at first place I did not find a good python implementation of this project. . Next Steps: . Add drawing functionality by mouse, so you can choose easily the starting config (done) | I think of optimizing the game by using numpy instead of grid (done) | Make an interactive web app of it using flask and deploy it | . Go back up .",
            "url": "/Blog/programming/2020/03/21/gameoflife.html",
            "relUrl": "/programming/2020/03/21/gameoflife.html",
            "date": " • Mar 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Fares Lassoued, 23 yo from Tunisia. I’m a cs graduate with interest in Data science, so I’m teaching myself to be a better coder, data scientist and a good problem solver! . You can find me on: . Linekedin | Github | Kaggle | Quora | .",
          "url": "/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "Project Description Link(s) Category . “Fatal Police shootings in the US” dataset visualization | Wrangle,analyze and communicate findings of the Fatal police shootings in the US dataset from kaggle. | - repo - kaggle kernel | data analysis | . “WeRateDogs” twitter dataset wrangling | Wrangle and Analyze the WeRateDogs dataset from the tweet archive of twitter user @dog_rates | - repo | data analysis, machine learning | . A/B test for e-commerce company | Perform A/B test and analyze results to help a company decide between a new developed pageand an old one for their e-commerce website. | - repo | data analysis, a/b testing | . Soccer database analysis | Data analysis and exploration of the european soccer database from kaggle. | - repo | data analysis | . Explore weather trends | Analyze local and global temperature data and compare the temperature trends where I live to overall global temperature trends. | - repo | data analysis | . MNIST guai | Desktop app for handwritten digit recognition using a machine learning classifier. I wanted to build on this ml project to test the classifier in real-time. | - repo | machine learning, | . Game of Life | This project represents the python Implementation of John Conway’s game of life using pygame library | - repo - blogpost | game programming | .",
          "url": "/Blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

}